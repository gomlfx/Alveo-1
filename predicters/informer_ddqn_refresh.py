# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f1ZvFGGS_qEguU48O3_ImcvAAorQ_Q5_
"""

# -*- coding: utf-8 -*-
"""informer_ddqn_sl_tp_1h_refresh

Adapted from original file at
    https://colab.research.google.com/drive/1y2eeNsEwlvFelWn2bG3EDrc7MQfZccL9

Modified to use DDQN instead of DQN for RL training.
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
from collections import deque
import random
import copy
import yfinance as yf
from datetime import datetime, timedelta
import time

# Define device at the top
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def get_positional_encoding(seq_len, d_model):
    pe = torch.zeros(seq_len, d_model)
    position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    return pe.unsqueeze(0)

# Informer Components
class ProbSparseAttention(nn.Module):
    def __init__(self, d_model, nhead, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.nhead = nhead
        self.d_k = d_model // nhead
        self.scale = 1 / (self.d_k ** 0.5)
        self.dropout = nn.Dropout(dropout)
        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        self.out_linear = nn.Linear(d_model, d_model)

    def forward(self, x):
        batch_size, seq_len, d_model = x.shape
        q = self.q_linear(x).view(batch_size, seq_len, self.nhead, self.d_k).transpose(1, 2)
        k = self.k_linear(x).view(batch_size, seq_len, self.nhead, self.d_k).transpose(1, 2)
        v = self.v_linear(x).view(batch_size, seq_len, self.nhead, self.d_k).transpose(1, 2)
        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale
        top_k = int(seq_len * 0.1)
        threshold = torch.topk(scores, top_k, dim=-1)[0][..., -1, None]
        mask = scores >= threshold
        scores = scores.masked_fill(~mask, float('-inf'))
        attn = torch.softmax(scores, dim=-1)
        attn = self.dropout(attn)
        out = torch.matmul(attn, v).transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)
        return self.out_linear(out)

class InformerEncoderLayer(nn.Module):
    def __init__(self, d_model, nhead, dim_feedforward=512, dropout=0.1):
        super().__init__()
        self.attention = ProbSparseAttention(d_model, nhead, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, dim_feedforward),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(dim_feedforward, d_model)
        )
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x):
        attn_out = self.attention(x)
        x = self.norm1(x + self.dropout1(attn_out))
        ffn_out = self.ffn(x)
        x = self.norm2(x + self.dropout2(ffn_out))
        return x

class InformerModel(nn.Module):
    def __init__(self, input_size=4, d_model=128, nhead=8, num_layers=3, dropout_rate=0.1):
        super().__init__()
        self.embedding = nn.Linear(input_size, d_model)
        self.informer = nn.ModuleList([InformerEncoderLayer(d_model, nhead, dim_feedforward=512, dropout=dropout_rate) for _ in range(num_layers)])
        self.dropout = nn.Dropout(dropout_rate)
        self.fc = nn.Linear(d_model, 1)

    def forward(self, x, training=False):
        batch_size, seq_len, _ = x.shape
        out = self.embedding(x) + get_positional_encoding(seq_len, self.embedding.out_features).to(x.device)
        for layer in self.informer:
            out = layer(out)
        out = out[:, -1, :]
        out = self.dropout(out) if training else out
        out = self.fc(out)
        return out

def train_model(model, loader, epochs=100, lr=0.0005):
    criterion = nn.MSELoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10)
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch_x, batch_y in loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            optimizer.zero_grad()
            out = model(batch_x, training=True)
            loss = criterion(out, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        scheduler.step(total_loss / len(loader))
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(loader):.6f}")
    return model

def mc_dropout_confidence_interval(models, input_seq, scaler, n_samples=1000, alpha=0.05):
    predictions = []
    input_seq = input_seq.to(device)
    for model in models:
        model.eval()
        for _ in range(n_samples // len(models)):
            with torch.no_grad():
                pred = model(input_seq, training=True)
                pred_price = scaler.inverse_transform([[0, 0, 0, pred.cpu().item()]])[0][3]
                predictions.append(pred_price)
    predictions = np.array(predictions)
    lower = np.percentile(predictions, 100 * alpha / 2)
    upper = np.percentile(predictions, 100 * (1 - alpha / 2))
    mean = np.mean(predictions)
    return lower, upper, mean

class DQN(nn.Module):
    def __init__(self, state_size, action_size=3):
        super().__init__()
        self.fc1 = nn.Linear(state_size, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, action_size)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

class ReplayBuffer:
    def __init__(self, capacity=20000):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)

    def __len__(self):
        return len(self.buffer)

def train_rl(hybrid_ensemble, scaled_data, seq_length, scaler, episodes=2000, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.999, batch_size=64, lr=0.0005):
    state_size = 2  # Close, forecast
    action_size = 3
    replay_buffer = ReplayBuffer()
    q_network = DQN(state_size, action_size).to(device)
    target_network = copy.deepcopy(q_network).to(device)
    optimizer = torch.optim.AdamW(q_network.parameters(), lr=lr)
    criterion = nn.MSELoss()
    reward_history = deque(maxlen=100)  # For early stopping

    for episode in range(episodes):
        total_reward = 0
        position = 0
        cash = 10000
        position_duration = 0
        for t in range(seq_length, len(scaled_data) - 1):
            seq = torch.tensor(scaled_data[t - seq_length:t].reshape(1, seq_length, -1), dtype=torch.float32).to(device)
            with torch.no_grad():
                ensemble_forecasts = [model(seq, training=False).item() for model in hybrid_ensemble]
                forecast = np.mean(ensemble_forecasts)
            state = np.array([scaled_data[t, 3], forecast])
            state_tensor = torch.tensor(state, dtype=torch.float32).to(device)

            if random.random() < epsilon:
                action = random.randint(0, action_size - 1)
            else:
                with torch.no_grad():
                    action = torch.argmax(q_network(state_tensor.unsqueeze(0))).item()

            current_price = scaler.inverse_transform([[0, 0, 0, scaled_data[t, 3]]])[0][3]
            next_price = scaler.inverse_transform([[0, 0, 0, scaled_data[t + 1, 3]]])[0][3]
            reward = 0
            if action == 0 and position == 0:  # Buy
                position = 1
                buy_price = current_price
                position_duration = 1
            elif action == 1 and position == 1:  # Sell
                position = 0
                profit = (next_price - buy_price)
                reward = profit - 0.01 * position_duration
                cash += reward
                position_duration = 0
            elif action == 2:  # Hold
                if position == 1:
                    reward = (next_price - buy_price) * 0.01 - 0.001 * position_duration
                    position_duration += 1

            next_state = np.array([scaled_data[t + 1, 3], forecast])
            done = (t == len(scaled_data) - 2)
            replay_buffer.push(state, action, reward, next_state, done)
            total_reward += reward

            if len(replay_buffer) >= batch_size:
                batch = replay_buffer.sample(batch_size)
                states, actions, rewards, next_states, dones = zip(*batch)
                states = torch.tensor(np.array(states), dtype=torch.float32).to(device)
                next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(device)
                actions = torch.tensor(actions, dtype=torch.long).to(device).unsqueeze(1)
                rewards = torch.tensor(rewards, dtype=torch.float32).to(device)
                dones = torch.tensor(dones, dtype=torch.float32).to(device)

                q_values = q_network(states).gather(1, actions).squeeze()
                with torch.no_grad():
                    # DDQN update: Online network selects action, target network evaluates
                    next_actions = q_network(next_states).max(1)[1].unsqueeze(1)
                    next_q_values = target_network(next_states).gather(1, next_actions).squeeze()
                targets = rewards + gamma * next_q_values * (1 - dones)

                loss = criterion(q_values, targets)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

        reward_history.append(total_reward)
        epsilon = max(epsilon_min, epsilon * epsilon_decay)
        if episode % 50 == 0:
            target_network.load_state_dict(q_network.state_dict())
            print(f"Episode {episode}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.2f}")

        # Early stopping: Stop if reward stabilizes (change < 1% over 100 episodes)
        if len(reward_history) == 100 and episode >= 100:
            avg_reward_old = np.mean(list(reward_history)[:50])
            avg_reward_new = np.mean(list(reward_history)[50:])
            if avg_reward_old > 0 and abs((avg_reward_new - avg_reward_old) / avg_reward_old) < 0.01:
                print(f"Early stopping at episode {episode}: Reward stabilized (Old: {avg_reward_old:.2f}, New: {avg_reward_new:.2f})")
                break

    return q_network

def main():
    # Fetch 1-year raw OHLC data (1 year ago to current date)
    end_date = datetime.today().strftime('%Y-%m-%d')  # Current date
    start_date = (datetime.today() - timedelta(days=365)).strftime('%Y-%m-%d')  # 1 year ago
    ticker = 'EURUSD=X'
    df = yf.download(ticker, start=start_date, end=end_date, interval='1d')
    df = df[['Open', 'High', 'Low', 'Close']].dropna()
    df.index = pd.to_datetime(df.index)

    # Validate data
    if df.empty or len(df['Close']) < 30:
        print(f"Error: Insufficient data for {ticker}. Skipping this cycle.")
        return

    # Preprocessing (raw OHLC: 4 features)
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(df.values)

    seq_length = 30

    def create_sequences(data, seq_length):
        xs, ys = [], []
        for i in range(len(data) - seq_length):
            x = data[i:i + seq_length]
            y = data[i + seq_length, 3]  # Predict 'Close' (index 3)
            xs.append(x)
            ys.append(y)
        return np.array(xs), np.array(ys)

    X, y = create_sequences(scaled_data, seq_length)
    if len(X) < 10:  # Ensure enough sequences
        print(f"Error: Too few sequences ({len(X)}) for training. Skipping this cycle.")
        return

    train_size = int(0.7 * len(X))
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]

    class TimeSeriesDataset(Dataset):
        def __init__(self, X, y):
            self.X = torch.tensor(X, dtype=torch.float32)
            self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)

        def __len__(self):
            return len(self.X)

        def __getitem__(self, idx):
            return self.X[idx], self.y[idx]

    train_dataset = TimeSeriesDataset(X_train, y_train)
    test_dataset = TimeSeriesDataset(X_test, y_test)
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

    # Train Ensemble
    ensemble_size = 3
    hybrid_ensemble = []
    for seed in range(ensemble_size):
        torch.manual_seed(seed)
        informer_model = InformerModel(input_size=4).to(device)
        informer_model = train_model(informer_model, train_loader)
        hybrid_ensemble.append(informer_model)

    # Train RL with DDQN
    rl_model = train_rl(hybrid_ensemble, scaled_data, seq_length, scaler)

    # Forecast next day (tomorrow's close)
    tomorrow_date = (datetime.today() + timedelta(days=1)).strftime('%Y-%m-%d')
    last_seq = torch.tensor(scaled_data[-seq_length:].reshape(1, seq_length, -1), dtype=torch.float32).to(device)
    with torch.no_grad():
        ensemble_preds = [scaler.inverse_transform([[0, 0, 0, model(last_seq, training=False).cpu().item()]])[0][3] for model in hybrid_ensemble]
        hybrid_pred = np.mean(ensemble_preds)

    # Monte Carlo Dropout CI
    lower_ci, upper_ci, mc_mean = mc_dropout_confidence_interval(hybrid_ensemble, last_seq, scaler, n_samples=1000)

    # Current price (latest close)
    current_price = df['Close'].iloc[-1].item()  # Ensure scalar

    # Volatility-based Stop Loss and Take Profit
    recent_closes = df['Close'].iloc[-30:].values
    volatility = np.std(recent_closes).item()  # Ensure scalar
    risk_reward_ratio = 3  # Default 1:3

    # RL Trading Action
    last_scaled = scaled_data[-1]
    hybrid_pred_scaled = (hybrid_pred - scaler.data_min_[3]) / (scaler.data_max_[3] - scaler.data_min_[3])
    state = np.array([last_scaled[3], hybrid_pred_scaled])
    state_tensor = torch.tensor(state, dtype=torch.float32).to(device)
    action = torch.argmax(rl_model(state_tensor.unsqueeze(0))).item()
    actions = {0: "Buy", 1: "Sell", 2: "Hold"}

    # Set stop loss and take profit based on action
    stop_loss = None
    take_profit = None
    if actions[action] == "Buy":
        # Long: Stop loss below, take profit above
        stop_loss = current_price - volatility  # Risk = 1x volatility
        take_profit = current_price + (risk_reward_ratio * volatility)  # Reward = 3x risk
    elif actions[action] == "Sell":
        # Short: Stop loss above, take profit below
        stop_loss = current_price + volatility
        take_profit = current_price - (risk_reward_ratio * volatility)
    else:  # Hold
        # Neutral: Reference levels
        stop_loss = current_price - volatility
        take_profit = current_price + (risk_reward_ratio * volatility)

    # Adjust take profit for 1:5 if within CI
    max_take_profit = current_price + (5 * volatility) if actions[action] == "Buy" else current_price - (5 * volatility) if actions[action] == "Sell" else current_price + (5 * volatility)
    if actions[action] in ["Buy", "Sell"] and lower_ci <= max_take_profit <= upper_ci:
        take_profit = max_take_profit
        risk_reward_ratio = 5

    print(f"Hybrid Ensemble Predicted Close for {tomorrow_date}: {hybrid_pred:.5f}")
    print(f"Hybrid Monte Carlo Mean: {mc_mean:.5f}")
    print(f"Hybrid 95% Confidence Interval: [{lower_ci:.5f}, {upper_ci:.5f}]")
    print(f"Current Price: {current_price:.5f}")
    print(f"Suggested Stop Loss: {stop_loss:.5f}")
    print(f"Suggested Take Profit: {take_profit:.5f}")
    print(f"Risk:Reward Ratio: 1:{risk_reward_ratio}")
    print(f"RL Suggested Action for {tomorrow_date}: {actions[action]}")

    # Plot
    plt.figure(figsize=(12, 6))
    plt.plot(df.index, df['Close'], label='Historical')
    pred_date = pd.to_datetime(tomorrow_date)
    plt.plot(pred_date, hybrid_pred, 'ro', label='Predicted Close')
    plt.fill_between([pred_date], lower_ci, upper_ci, color='red', alpha=0.2, label='95% Confidence Interval')
    plt.axhline(y=stop_loss, color='red', linestyle='--', label='Stop Loss')
    plt.axhline(y=take_profit, color='green', linestyle='--', label='Take Profit')
    plt.axhline(y=current_price, color='blue', linestyle=':', label='Current Price')
    plt.title(f'EUR/USD Closes and Informer Forecast with CI, Stop Loss, Take Profit (Action: {actions[action]})')
    plt.xlabel('Date')
    plt.ylabel('Price')
    plt.legend()
    plt.grid(True)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(f'eurusd_informer_raw_forecast_{datetime.now().strftime("%Y%m%d_%H%M%S")}.png')
    print(f"Plot saved as 'eurusd_informer_raw_forecast_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png'")
    plt.close()

if __name__ == "__main__":
    while True:
        print(f"Running forecast at {datetime.now()}")
        try:
            main()
        except Exception as e:
            print(f"Error in main: {e}")
        print("Sleeping for 1 hour...")
        time.sleep(3600)  # 1 hour